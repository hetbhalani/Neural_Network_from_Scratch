
<p align="center">
  <img src="https://img.shields.io/badge/python-3.9+-blue?logo=python">
  <img src="https://img.shields.io/badge/Library-numpy-lightgrey">
  <img src="https://img.shields.io/badge/math-handwritten-ff69b4">
</p>

<h1 align="left">ğŸ§  Neural Network from Scratch</h1>
<p align="left">
  Built using <b>NumPy</b> and <b>PURE MATHS</b> â€” no frameworks, no high-level libraries.
</p>


## âœ¨ Overview

This project is a **fully connected neural network** implemented from scratch â€” no deep learning libraries, just **NumPy** and manually derived math. 

I built this to learn exactly how neural networks work under the hood by understanding and implementing **forward propagation**, **backpropagation**, and **gradient descent** step by step.

---

## ğŸ§® Architecture

- **Input Layer:** 784 neurons (MNIST 28Ã—28 pixels)
- **Hidden Layer:** 64 neurons with **ReLU**
- **Output Layer:** 10 neurons with **Softmax**
- **Loss:** Categorical Cross-Entropy

---
## ğŸ““Derivation (Screenshots)

![ss](./imgs/img1.jpeg)
![ss](./imgs/img2.jpeg)
![ss](./imgs/img3.jpeg)

## ğŸ§  Why Build From Scratch?

Learning by building *without* high-level libraries gives deep insight into:
- How gradients and backprop actually work ğŸ”
- How softmax + cross-entropy connect ğŸ§©
- Where numerical errors happen (and how to fix them) ğŸ§®

---
## ğŸ“Š Results

-   Accuracy: <b>84%
    
    

----------

## ğŸ¤ Letâ€™s Connect!

Iâ€™d love to hear from others also building from scratch or diving deep into ML fundamentals!

-   ğŸ“« [LinkedIn](https://www.linkedin.com/in/het-bhalani-20403b2a8/)
    
-   ğŸ¦ [Twitter/X](https://x.com/het_bhalani)
    
-   ğŸ¦¤[Kaggle](https://www.kaggle.com/hetbhalani9)

### Built with Math and Love ğŸ’™